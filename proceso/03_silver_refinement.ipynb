{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c142d2-0c9f-4e3d-9bd5-a5e351d61ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, to_date, when\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "# 1. Configuración de tablas\n",
    "bronze_hist_table = \"proyecto_divisas.bronze.historico_divisas\"\n",
    "bronze_api_table = \"proyecto_divisas.bronze.api_divisas_actual\"\n",
    "silver_table = \"proyecto_divisas.silver.divisas_limpias\"\n",
    "\n",
    "# 2. Lectura de fuentes Bronze\n",
    "df_hist = spark.read.table(bronze_hist_table)\n",
    "df_api = spark.read.table(bronze_api_table)\n",
    "\n",
    "# 3. Paso 2.1: Unificación (Estandarización de Esquemas)\n",
    "# El histórico CSV suele tener nombres de columnas distintos; los alineamos al esquema destino\n",
    "df_hist_aligned = df_hist.select(\n",
    "    col(\"Date\").alias(\"event_timestamp\"),\n",
    "    col(\"Series\").alias(\"currency_pair\"),\n",
    "    col(\"Price\").alias(\"price\")\n",
    ")\n",
    "\n",
    "df_api_aligned = df_api.select(\n",
    "    col(\"event_timestamp\"),\n",
    "    col(\"currency_pair\"),\n",
    "    col(\"price\")\n",
    ")\n",
    "\n",
    "# Unión de ambas fuentes\n",
    "df_union = df_hist_aligned.unionByName(df_api_aligned)\n",
    "\n",
    "# 4. Paso 2.2: Limpieza y Tipado de Datos\n",
    "df_cleaned = df_union \\\n",
    "    .filter(col(\"price\").isNotNull() & col(\"event_timestamp\").isNotNull()) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(DecimalType(18, 4))) \\\n",
    "    .withColumn(\"event_date\", to_date(col(\"event_timestamp\"))) \\\n",
    "    .dropDuplicates([\"event_timestamp\", \"currency_pair\"]) # Eliminación de duplicados técnicos\n",
    "\n",
    "# 5. Paso 2.3: Carga en Capa Silver\n",
    "# Usamos un esquema administrado por Unity Catalog en el contenedor 'silver'\n",
    "(df_cleaned.write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\") # En Silver solemos reconstruir o usar MERGE para asegurar limpieza total\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(silver_table))\n",
    "\n",
    "print(f\"✅ Capa Silver optimizada en {silver_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e238976a-abf5-4e01-adf6-88972d08d5fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, to_date\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "# 1. Configuración de tablas\n",
    "bronze_hist_table = \"proyecto_divisas.bronze.historico_divisas\"\n",
    "bronze_api_table = \"proyecto_divisas.bronze.api_divisas_actual\"\n",
    "silver_table = \"proyecto_divisas.silver.divisas_limpias\"\n",
    "\n",
    "# 2. Lectura de fuentes Bronze\n",
    "df_hist = spark.read.table(bronze_hist_table)\n",
    "df_api = spark.read.table(bronze_api_table)\n",
    "\n",
    "# 3. Estandarización de Esquemas\n",
    "# Corregimos: 'Series' -> 'Symbol' y 'Price' -> 'Close'\n",
    "df_hist_aligned = df_hist.select(\n",
    "    col(\"Date\").alias(\"event_timestamp\"),\n",
    "    col(\"Symbol\").alias(\"currency_pair\"), # 'Symbol' contiene \"COP=X\", \"MXN=X\", etc.\n",
    "    col(\"Close\").alias(\"price\")           # Usamos el precio de cierre\n",
    ")\n",
    "\n",
    "# Alineamos la API (asumiendo que tiene estos nombres)\n",
    "df_api_aligned = df_api.select(\n",
    "    col(\"event_timestamp\"),\n",
    "    col(\"currency_pair\"),\n",
    "    col(\"price\")\n",
    ")\n",
    "\n",
    "# Unión de ambas fuentes\n",
    "df_union = df_hist_aligned.unionByName(df_api_aligned)\n",
    "\n",
    "# 4. Limpieza y Tipado (Senior Practice)\n",
    "df_cleaned = df_union \\\n",
    "    .filter(col(\"price\").isNotNull() & col(\"event_timestamp\").isNotNull()) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(DecimalType(18, 4))) \\\n",
    "    .withColumn(\"event_date\", to_date(col(\"event_timestamp\"))) \\\n",
    "    .dropDuplicates([\"event_timestamp\", \"currency_pair\"])\n",
    "\n",
    "# 5. Carga en Capa Silver\n",
    "(df_cleaned.write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(silver_table))\n",
    "\n",
    "print(f\"✅ Capa Silver optimizada en {silver_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_silver_refinement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
