{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6399d7-bd33-4a8a-8965-21ed4098a1bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d85c61e-0af1-4833-a25a-c2cf99f18eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import current_timestamp, lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7abc522-0e05-4cdc-84de-8e7b867632b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Configuración de Activos y Destino\n",
    "# Usamos los tickers específicos de Yahoo Finance para los pares solicitados\n",
    "tickers_mapping = {\n",
    "    \"COP=X\": \"USD/COP\",\n",
    "    \"MXN=X\": \"USD/MXN\",\n",
    "    \"EURUSD=X\": \"EUR/USD\",\n",
    "    \"BRL=X\": \"BRL/USD\",\n",
    "    \"BTC-USD\": \"BTC/USD\"\n",
    "}\n",
    "target_table = \"proyecto_divisas.bronze.api_divisas_actual\"\n",
    "\n",
    "# 2. Extracción (Corre en el Driver)\n",
    "print(f\"Descargando datos para: {list(tickers_mapping.keys())}\")\n",
    "data = yf.download(list(tickers_mapping.keys()), period=\"1d\", interval=\"1m\")\n",
    "\n",
    "# 3. Transformación Básica en Pandas (Preparación para Spark)\n",
    "# Obtenemos solo los precios de cierre y reseteamos el índice para tener la fecha/hora\n",
    "df_pd = data['Close'].reset_index()\n",
    "\n",
    "# Convertimos de formato ancho (columnas por ticker) a formato largo (filas por ticker)\n",
    "df_melted = df_pd.melt(id_vars=['Datetime'], var_name='ticker_raw', value_name='price')\n",
    "\n",
    "# 4. Conversión a Spark DataFrame y Enriquecimiento\n",
    "df_spark = spark.createDataFrame(df_melted)\n",
    "\n",
    "# Aplicamos el mapeo de nombres de activos y metadatos de auditoría\n",
    "df_bronze_api = df_spark \\\n",
    "    .withColumn(\"currency_pair\", col(\"ticker_raw\")) \\\n",
    "        .replace(tickers_mapping, subset=[\"currency_pair\"]) \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"source_system\", lit(\"Yahoo Finance API\")) \\\n",
    "    .select(\n",
    "        col(\"Datetime\").alias(\"event_timestamp\"),\n",
    "        \"currency_pair\",\n",
    "        \"price\",\n",
    "        \"ingestion_timestamp\",\n",
    "        \"source_system\"\n",
    "    )\n",
    "\n",
    "# 5. Carga a Bronze (Append para mantener el histórico de llamadas a la API)\n",
    "(df_bronze_api.write\n",
    " .format(\"delta\")\n",
    " .mode(\"append\") \n",
    " .saveAsTable(target_table))\n",
    "\n",
    "print(f\"✅ Ingesta incremental completada exitosamente en {target_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_ingest_api_incremental",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
